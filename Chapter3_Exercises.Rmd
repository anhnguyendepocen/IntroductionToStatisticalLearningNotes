---
title: "Chapter 3 Exercises"
author: "Loren Serfass"
date: "12/31/2014"
output:
    html_document:
        toc: true
runtime: shiny
---

```{r}
library(ggplot2)
```


# Conceptual

1. Describe the null hypotheses to which the p-values given in Table 3.4
correspond. Explain what conclusions you can draw based on these
p-values. Your explanation should be phrased in terms of sales, TV,
radio, and newspaper, rather than in terms of the coefficients of the
linear model.  --- **The null hypothesis for the p-value corresponding
to the Intercept term is that the Intercept is 0 (and that the other coefficient is ???)...**


2. Carefully explain the differences between the KNN classifier and KNN
regression methods.  **In the KNN classifier, the K nearest neighbors to a point
are found, and the most prevalent class among these is the class predicted for the point.
In KNN regression, the K nearest neighbors are found, and their values are averaged
to find the predicted value for the point.**

3. Suppose we have a data set with five predictors, X 1 = GPA, X 2 = IQ,
X 3 = Gender (1 for Female and 0 for Male), X 4 = Interaction between
GPA and IQ, and X 5 = Interaction between GPA and Gender. The
response is starting salary after graduation (in thousands of dollars).
Suppose we use least squares to fit the model, and get β ˆ 0 = 50, β ˆ 1 =
20, β ˆ 2 = 0.07, β ˆ 3 = 35, β ˆ 4 = 0.01, β ˆ 5 = −10.

(a) Which answer is correct, and why?

i. For a fixed value of IQ and GPA, males earn more on average
than females.
ii. For a fixed value of IQ and GPA, females earn more on
average than males.
iii. For a fixed value of IQ and GPA, males earn more on average
than females provided that the GPA is high enough.
iv. For a fixed value of IQ and GPA, females earn more on
average than males provided that the GPA is high enough.

(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.

(c) True or false: Since the coefficient for the GPA/IQ interaction
term is very small, there is very little evidence of an interaction
effect. Justify your answer.

4. I collect a set of data (n = 100 observations) containing a single
predictor and a quantitative response. I then fit a linear regression
model to the data, as well as a separate cubic regression, i.e.
$Y = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2}^2 + \beta_{3}X_{3}^3 + \epsilon$.

(a) Suppose that the true relationship between X and Y is linear,
i.e. $Y = \beta_{0} + \beta_{1}X + \epsilon$. Consider the training residual sum of
squares (RSS) for the linear regression, and also the training
RSS for the cubic regression. Would we expect one to be lower
than the other, would we expect them to be the same, or is there
not enough information to tell? Justify your answer.
**RSS for the cubic model would most likely be lower than for the linear model.
Think of the linear model as a version of the cubic model with $\beta_{2}$ and $\beta_{3}$
constrained to 0.  Allowing $\beta_{2}$ and $\beta_{3}$ to stray from 0 can further
reduce the training RSS.  Increasing the model's flexibility (degrees of freedom?)
will generally reduce training-set RSS. A simulation with only 10 points for clarity:**

```{r}
df <- data.frame(x = 1:20, y = 3 + (1:20) + rnorm(20, sd=3)) # a linear relationship plus error
test <- sample(20, 10)
lm1 <- lm(y ~ x, data=df[-test,])
lm2 <- lm(y ~ x + I(x^2) + I(x^3), data=df[-test,])
shape <- rep(1, times=20) # training set O
shape[test] <- 4 # test set X
plot(y ~ x, data=df, pch=shape)
abline(lm1)
x <- seq(1,20,by=.1)
lines(x,predict(lm2, data.frame(x=x)),col='red') # the cubic curve is closer to the points
sum(lm1$residuals^2)
sum(lm2$residuals^2) # just as I said, this is lower.
```


(b) Answer (a) using test rather than training RSS.
**The answer: $RSS_{test cubic} > RSS_{test linear}$. A cubic model would overfit the
data.**

```{r}
resid.test.lm1 <- df$y[test] - predict(lm1, newdata=df[test,])
resid.test.lm2 <- df$y[test] - predict(lm2, newdata=df[test,])
sum(resid.test.lm1^2) # as I said, this is lower.
sum(resid.test.lm2^2)
```


(c) Suppose that the true relationship between X and Y is not linear,
but we don’t know how far it is from linear. Consider the training
RSS for the linear regression, and also the training RSS for the
cubic regression. Would we expect one to be lower than the
other, would we expect them to be the same, or is there not
enough information to tell? Justify your answer.
**The $RSS_{cubic,training}$ is always lower than the $RSS_{linear,training}.$**

(d) Answer (c) using test rather than training RSS.
**There is not enough information. If the true relationship is quadratic,
then one model will overfit and the other will underfit, so they'll both have high RSS
but we don't know which will be higher.  If the relationship is cubic or quartic, then
the cubic model will have lower RSS on the test set.**

5. Consider the fitted values that result from performing linear regres-
sion without an intercept. In this setting, the ith fitted value takes
the form $$\hat{y}_{i} = x_{i}\hat{\beta},$$ where
$$\hat{\beta} = \left(\sum\limits_{i=1}^n x_{i}y_{i}\right)  /
                \left(\sum\limits_{i'=1}^n x_{i'}^2\right).$$
Show that we can write
$$\hat{y}_{i} = \sum\limits_{i'=1}^n a_{i'}y_{i'}.$$
What is $a_{i'}$?
*Note: We interpret this result by saying that the fitted values from
linear regression are linear combinations of the response values.*

6. Using (3.4), argue that in the case of simple linear regression, the
least squares line always passes through the point $(\bar{x}, \bar{y})$.

**ANSWER

Equation (3.4) is this combination:
$$\hat{\beta}_{1} = \frac{ \sum\limits_{i=1}^n(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum\limits_{i=1}^n(x_{i}-\bar{x})^2}$$
$$\hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x}$$

The regression equation is:
$\hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1}x$. If and only if $pred(\bar{x}) = \bar{y}$, then
substituting the above values of $\hat{\beta}_{0}$, $\bar{x}$,
and $\bar{y}$ into the regression equation should eventually reduce to an equality:

$$\bar{y} =  \left(\bar{y} - \hat{\beta}_{1}\bar{x}\right) + \hat{\beta}_{1}\bar{x}$$

Subtract $\bar{y}$ and combine terms on the right-hand side, and you get:

$$0 = 0.$$

